{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb42eb8d-7d55-40d1-9aa4-464968482209",
   "metadata": {},
   "source": [
    "# Benchmark Clustering\n",
    "\n",
    "This notebook compares the relative speed and accuracy of the different clustering algorithms. It can be used both to demonstrate how those algorithms work and to compare their performance. See the next section for the definition of the various parameters that we would expect to impact the performance of the clustering.\n",
    "\n",
    "**Note:** The notebook inserts fake **linear** trajectories, so it will only indicate the performance of clustering those. For objects whose movement is very nonlinear, we might see better performance from other clustering algorithms. However, since KBMOD searches for linear trajectories, it makes sense to evaluate the clustering algorithm relative to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02c346-0ef7-4774-b156-9f38002e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "\n",
    "from kbmod.configuration import SearchConfiguration\n",
    "from kbmod.fake_data.fake_data_creator import create_fake_times, FakeDataSet\n",
    "from kbmod.filters.clustering_filters import apply_clustering\n",
    "from kbmod.results import Results\n",
    "from kbmod.run_search import SearchRunner\n",
    "from kbmod.search import Trajectory\n",
    "from kbmod.trajectory_generator import VelocityGridSearch\n",
    "from kbmod.trajectory_utils import match_trajectory_sets\n",
    "\n",
    "import cProfile\n",
    "import timeit\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1f651-74d9-41e8-8964-fb70eec938c4",
   "metadata": {},
   "source": [
    "## Define critical parameters\n",
    "\n",
    "We predefine a few parameters that we expect to have a large impact on the performance of the clustering algorithm. Users may want to vary these to determine their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82c6cf-7077-41fb-830e-a44e07ca20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set sizes.  Larger image sizes will mean more potential results found (including noise).\n",
    "num_times = 20\n",
    "width = 600\n",
    "height = 500\n",
    "\n",
    "# Number of trajectories to insert.\n",
    "num_trjs = 20\n",
    "\n",
    "# The number of results returned per pixel.\n",
    "results_per_pixel = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0f54-90d6-4490-be35-429536fc902e",
   "metadata": {},
   "source": [
    "## Create a fake set of images with inserted trajectories\n",
    "\n",
    "We create a data set that initially consists of empty, noisy images. We sample 5 observations per night (spaced by ~30 minutes) on 4 consecutive nights for a total of 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba0743-76e4-4b76-b743-016c96b7ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake times with 5 observations per night and 20 total.\n",
    "times = create_fake_times(\n",
    "    num_times, t0=0.0, obs_per_day=5, intra_night_gap=0.02, inter_night_gap=1\n",
    ")\n",
    "\n",
    "# Create a fake data set.\n",
    "psf_val = 1.0\n",
    "fake_ds = FakeDataSet(\n",
    "    width, height, times, noise_level=1.0, psf_val=psf_val, use_seed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758b509-3cd1-4920-809b-60990d1cc697",
   "metadata": {},
   "source": [
    "Create a series of fake trajectories. Since we want these all to be detectable, we keep them to the left half (x <= width/2) and the middle of the chip (height/4 <= y <= 3*height/4). We then set the velocities so vx is always positive and vy can be positive or negative. Each true trajectory is inserted into each image and saved to a list for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b6066-7494-4212-91c2-9edbf58b66b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Sampling x from [0, {int(width/2)}]\")\n",
    "print(f\"Sampling y from [{int(height/4)}, {int(3*height/4)}]\")\n",
    "\n",
    "all_trjs = []\n",
    "for i in range(num_trjs):\n",
    "    trj = Trajectory(\n",
    "        x=int(width / 2 * rng.random()),\n",
    "        y=int(height / 2 * rng.random() + height / 4),\n",
    "        vx=20.0 * rng.random(),\n",
    "        vy=20.0 * rng.random() - 10.0,\n",
    "        flux=200,\n",
    "    )\n",
    "    print(f\"Trajectory {i}: {trj}\")\n",
    "\n",
    "    # Insert the object into the images.\n",
    "    fake_ds.insert_object(trj)\n",
    "\n",
    "    # Save the trajectory to the list.\n",
    "    all_trjs.append(trj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849af398-eb57-457a-9b36-a529febe6233",
   "metadata": {},
   "source": [
    "## Perform the search\n",
    "\n",
    "The search itself will use a grid of velocities where x=[0.0, 20.0] in 41 steps and y=[-10.0, 10.0] in 41 steps for a total of 1681 candidates per pixel.  The starting x and y locations for the search include all pixels in the images.\n",
    "\n",
    "Very minimal filtering is done to remove trajectories that are not visible in at least half the observations (`min_obs` parameter) or have negative likleihood (`lh_level` parameter). We also increase the maximum likelihood threshold. GPU filtering and masking are both turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f36a9-f681-4f0e-93bf-97bfa3f97a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_obs = int(num_times / 2)\n",
    "\n",
    "input_parameters = {\n",
    "    \"chunk_size\": 10_000_000,\n",
    "    \"do_mask\": False,\n",
    "    \"gpu_filter\": False,\n",
    "    \"lh_level\": 0.00000001,\n",
    "    \"max_lh\": 100000.0,\n",
    "    \"num_obs\": min_obs,\n",
    "    \"psf_val\": psf_val,\n",
    "    \"results_per_pixel\": results_per_pixel,\n",
    "    \"sigmaG_lims\": [25, 75],\n",
    "}\n",
    "config = SearchConfiguration.from_dict(input_parameters)\n",
    "\n",
    "# Create the search trajectories.\n",
    "trj_generator = VelocityGridSearch(41, 0.0, 20.0, 41, -10.0, 10.0)\n",
    "candidates = [trj for trj in trj_generator]\n",
    "print(f\"Testing {len(candidates)} trajectories per pixel.\")\n",
    "\n",
    "# Do the actual search.\n",
    "search = SearchRunner()\n",
    "results = search.do_gpu_search(config, fake_ds.stack, trj_generator)\n",
    "print(f\"Ran search and generated {len(results)} results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de4d3b-c731-4cdf-9e6c-7aa015660c63",
   "metadata": {},
   "source": [
    "For each of the trajectories inserted (true fakes), find the closest matching result that is no more than 5.0 pixels away on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f18d51-04e8-4150-9e7d-fcb7ae418396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_match_stats(all_trjs, results, threshold, times):\n",
    "    found_trjs = results.make_trajectory_list()\n",
    "    all_matches = match_trajectory_sets(all_trjs, found_trjs, threshold, times=times)\n",
    "    num_found = np.count_nonzero(all_matches > -1)\n",
    "    num_missed = len(all_trjs) - num_found\n",
    "    return num_found, num_missed\n",
    "\n",
    "\n",
    "num_found, num_missed = _compute_match_stats(all_trjs, results, 5.0, times)\n",
    "print(\n",
    "    f\"Matched {num_found} (and missed {num_missed}) of {len(all_trjs)} inserted trajectories.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112f01e-9c46-4454-b0e0-ae982ec36d05",
   "metadata": {},
   "source": [
    "## Run and evaluate clustering algorithms\n",
    "\n",
    "Iterate through the different clustering algorithms along with their threshold parameter. Each of these clustering calls takes a while, so we print out progress markers for each run.\n",
    "\n",
    "**Note:** Timing only uses a single clustering run (instead of the average of a bunch), so it will be noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57717dc5-b827-4ff5-90be-9d48aadbf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the clustering parameters\n",
    "cluster_params = {\n",
    "    \"cluster_type\": \"all\",\n",
    "    \"cluster_eps\": 20.0,\n",
    "    \"cluster_v_scale\": 1.0,\n",
    "    \"times\": np.array(times),\n",
    "}\n",
    "\n",
    "type_vals = [\n",
    "    \"all\",\n",
    "    \"position\",\n",
    "    \"mid_position\",\n",
    "    \"start_end_position\",\n",
    "    \"nn_start_end\",\n",
    "    \"nn_start\",\n",
    "]\n",
    "cluster_eps_vals = [1.0, 2.0, 5.0, 10.0, 20.0]\n",
    "\n",
    "stats_dict = {\n",
    "    \"name\": [],\n",
    "    \"eps\": [],\n",
    "    \"total\": [],\n",
    "    \"num_found\": [],\n",
    "    \"num_missed\": [],\n",
    "    \"run_time\": [],\n",
    "}\n",
    "for type_name in type_vals:\n",
    "    for eps in cluster_eps_vals:\n",
    "        print(f\"Testing '{type_name}': {eps}\")\n",
    "        cluster_params[\"cluster_type\"] = type_name\n",
    "        cluster_params[\"cluster_eps\"] = eps\n",
    "\n",
    "        # Make a temporary copy of the results for filtering.\n",
    "        tmp_res = results.copy()\n",
    "\n",
    "        # Do the clustering.\n",
    "        run_time = timeit.timeit(\n",
    "            \"apply_clustering(tmp_res, cluster_params)\", number=1, globals=globals()\n",
    "        )\n",
    "\n",
    "        # Score the results.\n",
    "        num_found, num_missed = _compute_match_stats(all_trjs, tmp_res, 5.0, times)\n",
    "        stats_dict[\"name\"].append(type_name)\n",
    "        stats_dict[\"eps\"].append(eps)\n",
    "        stats_dict[\"total\"].append(len(tmp_res))\n",
    "        stats_dict[\"num_found\"].append(num_found)\n",
    "        stats_dict[\"num_missed\"].append(num_missed)\n",
    "        stats_dict[\"run_time\"].append(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7938f-ba02-47ae-b1bf-8a3ceac4a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table out in blocks by clustering algorithm.\n",
    "tbl = Table(stats_dict)\n",
    "for type_name in type_vals:\n",
    "    print(f\"\\nData for clustering={type_name}\")\n",
    "    print(tbl[tbl[\"name\"] == type_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56372f3-fcbb-4f98-958b-dbfd873f1603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jeremy's KBMOD",
   "language": "python",
   "name": "kbmod_jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
